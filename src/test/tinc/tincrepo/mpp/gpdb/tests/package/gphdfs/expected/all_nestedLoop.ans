-- start_ignore
-- end_ignore
\echo --start_ignore
--start_ignore
drop external table all_heap;
DROP EXTERNAL TABLE
drop external table all_readhdfs_mapreduce;
DROP EXTERNAL TABLE
drop external table all_readhdfs_mapred;
DROP EXTERNAL TABLE
\echo --end_ignore
--end_ignore
create readable external table all_heap(
datatype_all varchar, xcount_all bigint,
col1_time time,col2_time time, col3_time time, col4_time time, col5_time time, col6_time time, col7_time time, col8_time time, col9_time time, nullcol_time time,
col1_timestamp timestamp,col2_timestamp timestamp, col3_timestamp timestamp, nullcol_timestamp timestamp,
col1_date date,col2_date date, col3_date date, col4_date date, col5_date date, col6_date date, col7_date date, nullcol_date date,
max_bigint bigint, min_bigint bigint, x_bigint bigint, reverse_bigint bigint, increment_bigint bigint, nullcol_bigint bigint,
max_int int, min_int int, x_int int, reverse_int int, increment_int int, nullcol_int int,
max_smallint smallint, min_smallint smallint, x_smallint smallint, reverse_smallint smallint, increment_smallint smallint, nullcol_smallint smallint,
max_real real, min_real real, pi_real real, piX_real real, nullcol_real real,
max_float float, min_float float, pi_float float, piX_float float, nullcol_float float,
col1_boolean boolean, nullcol_boolean boolean,
col1_varchar varchar,col2_varchar varchar, nullcol_varchar varchar,
col1_bpchar bpchar,col2_bpchar bpchar, nullcol_bpchar bpchar,
max_numeric numeric, min_numeric numeric, x_numeric numeric, reverse_numeric numeric, increment_numeric numeric, nullcol_numeric numeric,
col1_text text,col2_text text, nullcol_text text
) location ('gpfdist://sdw1.dh.greenplum.com:8080/all.txt')format 'TEXT'(ESCAPE  'OFF');
CREATE EXTERNAL TABLE
\!export HADOOP_HOME=/usr/lib/gphd/hadoop/; source /data/gpadmin/gpdb/greenplum/lib/hadoop/hadoop_env.sh; java -cp .:$CLASSPATH:/data/gpadmin/gpdb/greenplum/lib/hadoop/gphd-2.0.2-gnet-1.2.0.0.jar:/data/gpadmin/workspace/tincrepo/main/mpp/gpdb/tests/package/gphdfs/maptest.jar -Dhdfshost=sdw1.dh.greenplum.com -Ddatanodeport=8020 -Djobtrackerhost=sdw1.dh.greenplum.com -Djobtrackerport=8020  -DcompressionType=none javaclasses/TestHadoopIntegration mapreduce Mapreduce_mapper_TextIn /plaintext/all.txt /mapreduce/all 
14/08/15 12:50:34 INFO Configuration.deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS
14/08/15 12:50:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
14/08/15 12:50:35 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
14/08/15 12:50:36 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 125 for gpadmin on 192.168.2.100:8020
14/08/15 12:50:36 INFO security.TokenCache: Got dt for hdfs://sdw1.dh.greenplum.com:8020; Kind: HDFS_DELEGATION_TOKEN, Service: 192.168.2.100:8020, Ident: (HDFS_DELEGATION_TOKEN token 125 for gpadmin)
14/08/15 12:50:36 WARN mapreduce.JobSubmitter: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
14/08/15 12:50:36 INFO input.FileInputFormat: Total input paths to process : 1
14/08/15 12:50:36 INFO mapreduce.JobSubmitter: number of splits:1
14/08/15 12:50:36 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name
14/08/15 12:50:36 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
14/08/15 12:50:36 INFO Configuration.deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS
14/08/15 12:50:36 INFO Configuration.deprecation: mapred.job.reduce.memory.mb is deprecated. Instead, use mapreduce.reduce.memory.mb
14/08/15 12:50:36 INFO Configuration.deprecation: mapred.output.value.class is deprecated. Instead, use mapreduce.job.output.value.class
14/08/15 12:50:36 INFO Configuration.deprecation: mapreduce.map.class is deprecated. Instead, use mapreduce.job.map.class
14/08/15 12:50:36 INFO Configuration.deprecation: mapred.job.name is deprecated. Instead, use mapreduce.job.name
14/08/15 12:50:36 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address
14/08/15 12:50:36 INFO Configuration.deprecation: mapreduce.inputformat.class is deprecated. Instead, use mapreduce.job.inputformat.class
14/08/15 12:50:36 INFO Configuration.deprecation: mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir
14/08/15 12:50:36 INFO Configuration.deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
14/08/15 12:50:36 INFO Configuration.deprecation: mapreduce.outputformat.class is deprecated. Instead, use mapreduce.job.outputformat.class
14/08/15 12:50:36 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
14/08/15 12:50:36 INFO Configuration.deprecation: mapred.output.key.class is deprecated. Instead, use mapreduce.job.output.key.class
14/08/15 12:50:36 INFO Configuration.deprecation: mapred.job.map.memory.mb is deprecated. Instead, use mapreduce.map.memory.mb
14/08/15 12:50:36 INFO Configuration.deprecation: mapred.working.dir is deprecated. Instead, use mapreduce.job.working.dir
14/08/15 12:50:36 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1408131000469_0023
14/08/15 12:50:36 INFO mapreduce.JobSubmitter: Kind: HDFS_DELEGATION_TOKEN, Service: 192.168.2.100:8020, Ident: (HDFS_DELEGATION_TOKEN token 125 for gpadmin)
14/08/15 12:50:37 INFO impl.YarnClientImpl: Submitted application application_1408131000469_0023 to ResourceManager at /0.0.0.0:8032
14/08/15 12:50:37 INFO mapreduce.Job: The url to track the job: http://0.0.0.0:9999/proxy/application_1408131000469_0023/
14/08/15 12:50:37 INFO mapreduce.Job: Running job: job_1408131000469_0023
14/08/15 12:50:46 INFO mapreduce.Job: Job job_1408131000469_0023 running in uber mode : false
14/08/15 12:50:46 INFO mapreduce.Job:  map 0% reduce 0%
14/08/15 12:50:52 INFO mapreduce.Job:  map 100% reduce 0%
14/08/15 12:50:59 INFO mapreduce.Job:  map 100% reduce 100%
14/08/15 12:50:59 INFO mapreduce.Job: Job job_1408131000469_0023 completed successfully
14/08/15 12:50:59 INFO mapreduce.Job: Counters: 43
	File System Counters
		FILE: Number of bytes read=7094926
		FILE: Number of bytes written=14359309
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=6727989
		HDFS: Number of bytes written=7165006
		HDFS: Number of read operations=6
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Launched reduce tasks=1
		Data-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=13137
		Total time spent by all reduces in occupied slots (ms)=12747
	Map-Reduce Framework
		Map input records=5000
		Map output records=5000
		Map output bytes=7074920
		Map output materialized bytes=7094926
		Input split bytes=116
		Combine input records=0
		Combine output records=0
		Reduce input groups=1
		Reduce shuffle bytes=7094926
		Reduce input records=5000
		Reduce output records=5000
		Spilled Records=10000
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=83
		CPU time spent (ms)=4970
		Physical memory (bytes) snapshot=1031741440
		Virtual memory (bytes) snapshot=6098264064
		Total committed heap usage (bytes)=1373372416
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=6727873
	File Output Format Counters 
		Bytes Written=7165006
create readable external table all_readhdfs_mapreduce(like all_heap) location ('gphdfs://sdw1.dh.greenplum.com:8020/mapreduce/all')format 'custom' (formatter='gphdfs_import');
CREATE EXTERNAL TABLE
\!export HADOOP_HOME=/usr/lib/gphd/hadoop/; source /data/gpadmin/gpdb/greenplum/lib/hadoop/hadoop_env.sh; java -cp .:$CLASSPATH:/data/gpadmin/gpdb/greenplum/lib/hadoop/gphd-2.0.2-gnet-1.2.0.0.jar:/data/gpadmin/workspace/tincrepo/main/mpp/gpdb/tests/package/gphdfs/maptest.jar -Dhdfshost=sdw1.dh.greenplum.com -Ddatanodeport=8020 -Djobtrackerhost=sdw1.dh.greenplum.com -Djobtrackerport=8020  -DcompressionType=none javaclasses/TestHadoopIntegration mapred Mapred_mapper_TextIn /plaintext/all.txt /mapred/all
14/08/15 12:51:00 INFO Configuration.deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS
14/08/15 12:51:00 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address
14/08/15 12:51:00 INFO Configuration.deprecation: mapred.job.map.memory.mb is deprecated. Instead, use mapreduce.map.memory.mb
14/08/15 12:51:00 INFO Configuration.deprecation: mapred.job.reduce.memory.mb is deprecated. Instead, use mapreduce.reduce.memory.mb
14/08/15 12:51:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
14/08/15 12:51:02 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
14/08/15 12:51:02 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
14/08/15 12:51:02 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 126 for gpadmin on 192.168.2.100:8020
14/08/15 12:51:02 INFO security.TokenCache: Got dt for hdfs://sdw1.dh.greenplum.com:8020; Kind: HDFS_DELEGATION_TOKEN, Service: 192.168.2.100:8020, Ident: (HDFS_DELEGATION_TOKEN token 126 for gpadmin)
14/08/15 12:51:03 WARN mapreduce.JobSubmitter: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
14/08/15 12:51:03 INFO mapred.FileInputFormat: Total input paths to process : 1
14/08/15 12:51:03 INFO mapreduce.JobSubmitter: number of splits:2
14/08/15 12:51:03 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name
14/08/15 12:51:03 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
14/08/15 12:51:03 INFO Configuration.deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS
14/08/15 12:51:03 INFO Configuration.deprecation: mapred.output.value.class is deprecated. Instead, use mapreduce.job.output.value.class
14/08/15 12:51:03 INFO Configuration.deprecation: mapred.job.name is deprecated. Instead, use mapreduce.job.name
14/08/15 12:51:03 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address
14/08/15 12:51:03 INFO Configuration.deprecation: mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir
14/08/15 12:51:03 INFO Configuration.deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
14/08/15 12:51:03 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
14/08/15 12:51:03 INFO Configuration.deprecation: mapred.output.key.class is deprecated. Instead, use mapreduce.job.output.key.class
14/08/15 12:51:03 INFO Configuration.deprecation: mapred.working.dir is deprecated. Instead, use mapreduce.job.working.dir
14/08/15 12:51:03 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1408131000469_0024
14/08/15 12:51:03 INFO mapreduce.JobSubmitter: Kind: HDFS_DELEGATION_TOKEN, Service: 192.168.2.100:8020, Ident: (HDFS_DELEGATION_TOKEN token 126 for gpadmin)
14/08/15 12:51:03 INFO impl.YarnClientImpl: Submitted application application_1408131000469_0024 to ResourceManager at /0.0.0.0:8032
14/08/15 12:51:03 INFO mapreduce.Job: The url to track the job: http://0.0.0.0:9999/proxy/application_1408131000469_0024/
14/08/15 12:51:03 INFO mapreduce.Job: Running job: job_1408131000469_0024
14/08/15 12:51:15 INFO mapreduce.Job: Job job_1408131000469_0024 running in uber mode : false
14/08/15 12:51:15 INFO mapreduce.Job:  map 0% reduce 0%
14/08/15 12:51:21 INFO mapreduce.Job:  map 100% reduce 0%
14/08/15 12:51:28 INFO mapreduce.Job:  map 100% reduce 100%
14/08/15 12:51:28 INFO mapreduce.Job: Job job_1408131000469_0024 completed successfully
14/08/15 12:51:28 INFO mapreduce.Job: Counters: 43
	File System Counters
		FILE: Number of bytes read=7094926
		FILE: Number of bytes written=14443529
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=6731055
		HDFS: Number of bytes written=7165006
		HDFS: Number of read operations=9
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=2
		Launched reduce tasks=1
		Data-local map tasks=2
		Total time spent by all maps in occupied slots (ms)=25689
		Total time spent by all reduces in occupied slots (ms)=13041
	Map-Reduce Framework
		Map input records=5000
		Map output records=5000
		Map output bytes=7074920
		Map output materialized bytes=7094932
		Input split bytes=206
		Combine input records=0
		Combine output records=0
		Reduce input groups=1
		Reduce shuffle bytes=7094932
		Reduce input records=5000
		Reduce output records=5000
		Spilled Records=10000
		Shuffled Maps =2
		Failed Shuffles=0
		Merged Map outputs=2
		GC time elapsed (ms)=143
		CPU time spent (ms)=6670
		Physical memory (bytes) snapshot=1830756352
		Virtual memory (bytes) snapshot=8288530432
		Total committed heap usage (bytes)=2505703424
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=6730849
	File Output Format Counters 
		Bytes Written=7165006
create readable external table all_readhdfs_mapred(like all_readhdfs_mapreduce) location ('gphdfs://sdw1.dh.greenplum.com:8020/mapred/all')format 'custom' (formatter='gphdfs_import');
CREATE EXTERNAL TABLE
select count(*) from all_readhdfs_mapreduce;
 count 
-------
  5000
(1 row)

select count(*) from all_readhdfs_mapred;
 count 
-------
  5000
(1 row)

set enable_nestloop = on;
SET
set enable_hashjoin = off;
SET
--explain select count(a.x_numeric) from all_readhdfs_mapreduce a, all_readhdfs_mapred b where a.xcount_all > 100 and a.xcount_all < 105 and a.x_bigint = b.x_bigint;
select count(a.x_numeric) from all_readhdfs_mapreduce a, all_readhdfs_mapred b where a.xcount_all > 100 and a.xcount_all < 105 and a.x_bigint = b.x_bigint;
 count 
-------
     4
(1 row)

set enable_nestloop = off;
SET
set enable_hashjoin = on;
SET
